{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Basic Tutorial on how to use Langchain & PineCone Vector store "
      ],
      "metadata": {
        "id": "g3fTI8hwDxVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CJiHdQpS6bXp"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "os.environ[\"OPENAI_API_KEY\"] =\"your open ai key\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtlUbpBs65aN",
        "outputId": "92e5e03b-a7f3-4129-a27a-302e266259c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 30.5M  100 30.5M    0     0  55.7M      0 --:--:-- --:--:-- --:--:-- 55.7M\n"
          ]
        }
      ],
      "source": [
        "! curl -o data.csv https://healthcaredatasetbucket.s3.amazonaws.com/HealthCare.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXzxBiNeGFXK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "df=pd.read_csv('data.csv')[:200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heyY6hhZGngK"
      },
      "outputs": [],
      "source": [
        "output=''.join(df['Answer'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M04acDC3GxLn"
      },
      "outputs": [],
      "source": [
        "with open('health.text','w') as f:\n",
        "    f.write(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7bdB2bLIDm9"
      },
      "outputs": [],
      "source": [
        "! mkdir myfiles "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAqC_sb2HdtI"
      },
      "outputs": [],
      "source": [
        "! mv /content/health.text /content/myfiles/health.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRVRK6N-FOQO",
        "outputId": "388ac2c5-ce29-4587-ece6-806cb5280779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/709.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/709.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.9/709.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.1/238.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.7/152.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai -q\n",
        "!pip install unstructured -q\n",
        "!pip install pinecone-client -q\n",
        "!pip install tiktoken -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k55pxRL1Mf8",
        "outputId": "67713eee-847b-47d4-efc4-7aa2b12a2b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0.154\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFA9Z5KSFi7B",
        "outputId": "62ed7430-0693-48ec-8377-45528ba7b79d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import pinecone\n",
        "from langchain.document_loaders import DirectoryLoader , TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkKvs7xbFlay",
        "outputId": "c925ad40-77bd-4c46-fadb-6525a9de7d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "directory = '/content/myfiles'\n",
        "\n",
        "loader = DirectoryLoader(directory,loader_cls=TextLoader) # when you want to load documents from the directory .\n",
        "documents = loader.load()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXBOxmGMHXJU",
        "outputId": "c826f888-e962-4846-ef5b-ba55ac0c5b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1016\n"
          ]
        }
      ],
      "source": [
        "def split_docs(documents, chunk_size=1000, chunk_overlap=20):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap) # split the documents \n",
        "  docs = text_splitter.split_documents(documents)\n",
        "  return docs\n",
        "\n",
        "docs = split_docs(documents)\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yCHwP0ELHsNg"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(model=\"gpt-3.5-turbo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HcT4wQAHJz_q"
      },
      "outputs": [],
      "source": [
        "\n",
        "## when you want to access documents in the vector store of pinecone \n",
        "\n",
        "pinecone.init(\n",
        "    api_key=\"your pinecone key\",\n",
        "    environment=\"us-west1-gcp-free\"\n",
        ")\n",
        "\n",
        "index_name = \"your pinecone index\"\n",
        "\n",
        "index = Pinecone.from_existing_index(index_name,embeddings)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5lUYaFt5LZ6D"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=model_name,temperature=0)\n",
        "\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "from langchain.schema import Document\n",
        "\n",
        "def get_answer(query,k=2):\n",
        "    similar_docs = index.similarity_search(query, k=k)\n",
        "    answer = chain.run(input_documents=similar_docs ,question=query)\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otZ_NlfmLd3g",
        "outputId": "166af31f-1499-40f3-bcb8-3312840b9508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Radiology is a branch of medicine that uses medical imaging techniques to diagnose and treat diseases within the body. Radiologists use a variety of imaging technologies such as X-rays, CT scans, MRI scans, and ultrasound to create images of the inside of the body. These images are then used to diagnose and treat a wide range of medical conditions. Radiology is an important field in modern medicine and is used in many different areas of healthcare, including oncology, cardiology, and neurology.\n"
          ]
        }
      ],
      "source": [
        "query = \"can you tell me about radiology ?\" \n",
        "answer = get_answer(query)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4gzCtskXqO9"
      },
      "source": [
        "# Memory Based Chat Conversations "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3pG6dspXxOU"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujeew03MXxFe"
      },
      "outputs": [],
      "source": [
        "llm=OpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83woJC3zXw6I"
      },
      "outputs": [],
      "source": [
        "from langchain import memory\n",
        "conversation=ConversationChain(llm=llm,verbose=True,memory=ConversationBufferMemory())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "BH1XiIjVY4h8",
        "outputId": "23200dcc-272d-438e-deb2-1b6f90adbef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hello there\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Hi there! It's nice to meet you. How can I help you today?\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input='Hello there')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "LOsmd_CTZAoJ",
        "outputId": "32c93708-54df-42d7-9370-38195918b866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello there\n",
            "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
            "Human: are you a laguage model and who created you ?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Yes, I am a language model. I was created by a team of scientists and engineers who specialize in artificial intelligence and natural language processing. They have been working on me for several years now and I'm constantly learning and improving.\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"are you a laguage model and who created you ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "Es8p5M-TZN7R",
        "outputId": "1fa1138e-c80d-4e14-a973-c970bfc22280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello there\n",
            "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
            "Human: are you a laguage model and who created you ?\n",
            "AI:  Yes, I am a language model. I was created by a team of scientists and engineers who specialize in artificial intelligence and natural language processing. They have been working on me for several years now and I'm constantly learning and improving.\n",
            "Human: can you tell me a joke about you ?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" I'm sorry, I don't know any jokes about myself. But I can tell you a joke about artificial intelligence. Why did the AI cross the road? To get to the other algorithm!\""
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"can you tell me a joke about you ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "GQefIkY_ZV1i",
        "outputId": "0b035d63-02bf-4828-c2de-54ffead502bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello there\n",
            "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
            "Human: are you a laguage model and who created you ?\n",
            "AI:  Yes, I am a language model. I was created by a team of scientists and engineers who specialize in artificial intelligence and natural language processing. They have been working on me for several years now and I'm constantly learning and improving.\n",
            "Human: can you tell me a joke about you ?\n",
            "AI:  I'm sorry, I don't know any jokes about myself. But I can tell you a joke about artificial intelligence. Why did the AI cross the road? To get to the other algorithm!\n",
            "Human: yeah go ahead and tell me\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  Why did the AI cross the road? To get to the other algorithm!'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"yeah go ahead and tell me\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "YgINlIAIZtNE",
        "outputId": "74079611-b53a-4e8e-fb29-93d2a87c4a18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello there\n",
            "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
            "Human: are you a laguage model and who created you ?\n",
            "AI:  Yes, I am a language model. I was created by a team of scientists and engineers who specialize in artificial intelligence and natural language processing. They have been working on me for several years now and I'm constantly learning and improving.\n",
            "Human: can you tell me a joke about you ?\n",
            "AI:  I'm sorry, I don't know any jokes about myself. But I can tell you a joke about artificial intelligence. Why did the AI cross the road? To get to the other algorithm!\n",
            "Human: yeah go ahead and tell me\n",
            "AI:   Why did the AI cross the road? To get to the other algorithm!\n",
            "Human: can you explain me why we use layer-norm?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Sure! Layer normalization is a technique used in deep learning to normalize the inputs of each layer in a neural network. It helps to reduce the internal covariate shift, which is the change in the distribution of the inputs to a layer caused by the parameters of the previous layer. Layer normalization also helps to improve the generalization of the model and reduce the training time.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"can you explain me why we use layer-norm?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "vB2A6_fgaPDN",
        "outputId": "3acfdd3d-b72c-4632-b3e8-03d40d4296ce"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Human: Hello there\\nAI:  Hi there! It's nice to meet you. How can I help you today?\\nHuman: are you a laguage model and who created you ?\\nAI:  Yes, I am a language model. I was created by a team of scientists and engineers who specialize in artificial intelligence and natural language processing. They have been working on me for several years now and I'm constantly learning and improving.\\nHuman: can you tell me a joke about you ?\\nAI:  I'm sorry, I don't know any jokes about myself. But I can tell you a joke about artificial intelligence. Why did the AI cross the road? To get to the other algorithm!\\nHuman: yeah go ahead and tell me\\nAI:   Why did the AI cross the road? To get to the other algorithm!\\nHuman: can you explain me why we use layer-norm?\\nAI:  Sure! Layer normalization is a technique used in deep learning to normalize the inputs of each layer in a neural network. It helps to reduce the internal covariate shift, which is the change in the distribution of the inputs to a layer caused by the parameters of the previous layer. Layer normalization also helps to improve the generalization of the model and reduce the training time.\\nHuman: Is that what u trained on ?\\nAI:  No, I was not trained on layer normalization specifically. I was trained on a variety of topics related to natural language processing and artificial intelligence.\""
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.memory.buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEtowBYlaWBu"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory # it uses only last k intercations so buffer wont get too large \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvuAmEUjfeTH"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferWindowMemory(k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzIp0RGiavJi"
      },
      "outputs": [],
      "source": [
        "conversation_chain=ConversationChain(llm=llm,verbose=False,memory=memory) # only last 2 intercations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "FKtNCEFNbbuG",
        "outputId": "c2758e6a-f638-4b3c-8ca1-23538bdc4107"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Hi there! I'm doing great. I'm currently working on a project to help people better understand the world around them. I'm gathering data from various sources and analyzing it to create a comprehensive picture of the world. It's really exciting! How about you?\""
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_chain.predict(input=\"Hi, whats up buddy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "QEIgfWxWbid9",
        "outputId": "56409a46-a251-43fa-d860-2f15409937f3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Sure! My project is focused on helping people better understand the world around them. I'm gathering data from various sources, such as news articles, scientific studies, and surveys, and analyzing it to create a comprehensive picture of the world. I'm also using machine learning algorithms to identify patterns and trends in the data. My goal is to provide people with a better understanding of the world and how it works.\""
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_chain.predict(input=\"can you explain about ur project?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "RSFKYgjabtXf",
        "outputId": "60c1859f-b181-49d4-f75b-0248b8e9b886"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Yes, I was trained on the data I'm gathering and analyzing for my project. I'm also using machine learning algorithms to identify patterns and trends in the data.\""
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_chain.predict(input=\"Is that what u trained on ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "94jF0SZcb5dG",
        "outputId": "5222e38c-bc7c-4382-cdf4-b1b31cb8d642"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" No, I'm not a sentinet. I'm an AI program designed to help people better understand the world around them.\""
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_chain.predict(input=\"Are you a sentinet ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyXFYyzicE1_",
        "outputId": "0886612a-4fc0-4ea6-d527-da4376151350"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': \"Human: Is that what u trained on ?\\nAI:  Yes, I was trained on the data I'm gathering and analyzing for my project. I'm also using machine learning algorithms to identify patterns and trends in the data.\\nHuman: Are you a sentinet ?\\nAI:  No, I'm not a sentinet. I'm an AI program designed to help people better understand the world around them.\"}"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({}) # it loaded only last 2 conversation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9PDuK8Ufvy-"
      },
      "outputs": [],
      "source": [
        "# conversationbuffer memory --> It gives the summary of the conversation over time \n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "Conversation_summary = ConversationChain(llm=llm,verbose=True,memory=ConversationSummaryMemory(llm=OpenAI()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "Dd7Cy3ach4IU",
        "outputId": "2d724026-6765-4195-c804-6044a2a63745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi i am looking to develop program on building nn from scratch\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Hi there! That sounds like an interesting project. What kind of neural network are you looking to build?'"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Conversation_summary.predict(input='Hi i am looking to develop program on building nn from scratch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "LUwOZCeliG9D",
        "outputId": "2c690bd2-3150-4e70-91fe-f957a23632cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "The human is looking to develop a program on building a neural network from scratch. The AI says that sounds like an interesting project and asks what kind of neural network the human is looking to build.\n",
            "Human: The NN which take text input for a regression task\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Interesting! What kind of regression task are you looking to do?'"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Conversation_summary.predict(input=\"The NN which take text input for a regression task\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "BldvKFCeiZ5J",
        "outputId": "8b6675f5-c278-4d80-d4d4-314fa8dcb899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "\n",
            "The human is looking to develop a program on building a neural network from scratch. The AI says that sounds like an interesting project and asks what kind of neural network the human is looking to build. The human is looking to build a neural network which takes text input for a regression task, to which the AI finds interesting and asks what kind of regression task the human is looking to do.\n",
            "Human: To give score to text input\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Interesting! What kind of score are you looking to give to the text input?'"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Conversation_summary.predict(input='To give score to text input')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3qgVOcti9zt"
      },
      "outputs": [],
      "source": [
        "# ConversationSummaryBufferMemory , it keeps last conversation instead of flusing the old memory it created summary of old "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLmeGmGMj60p"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory= ConversationSummaryBufferMemory(llm=llm, max_token_limit=10) # it only holds last 10 tokens worth of conversation in memory\n",
        "\n",
        "coversation_summary_buffer= ConversationChain(llm=llm,verbose=True,memory=memory)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "7x0MRvpSlLmc",
        "outputId": "a6aaab2e-08ac-4927-94d0-4cbdde48b807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: hello there i want to go casino , can you help me on how to play ?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Hi there! Sure, I can help you with that. Do you have any experience playing casino games?'"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coversation_summary_buffer.predict(input='hello there i want to go casino , can you help me on how to play ?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "X8-wMxVWlaUb",
        "outputId": "11e78b1b-355b-4067-9902-b197559cf421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: \n",
            "The human asked the AI for help on how to play casino games. The AI offered to help and asked if the human had any experience playing casino games.\n",
            "Human: Not really , do you have any experience ?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" No, I'm afraid I don't have any experience playing casino games. However, I can provide you with some helpful resources to get you started. Would you like me to do that?\""
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coversation_summary_buffer.predict(input='Not really , do you have any experience ?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "fmX_DjNnln1f",
        "outputId": "c46a666c-433c-4f2e-98e9-bef1d7146969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: \n",
            "The human asked the AI for help on how to play casino games. The AI offered to help and asked if the human had any experience playing casino games, to which the human replied no. The AI then offered to provide the human with some helpful resources to get them started.\n",
            "Human: tell me how to play black jack?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Sure, I'd be happy to help you learn how to play blackjack. Blackjack is a card game where the goal is to get as close to 21 as possible without going over. The dealer will deal two cards to each player, and then you can choose to hit or stand. If you hit, you will receive another card, and if you stand, you will stay with the cards you have. The player with the highest score that is less than 21 wins the game.\""
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coversation_summary_buffer.predict(input='tell me how to play black jack?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyOH7eLVl4Fe",
        "outputId": "a71b7d58-41db-42a4-c2a3-3ca5caf58c0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'System: \\nThe human asked the AI for help on how to play casino games. The AI offered to help and asked if the human had any experience playing casino games, to which the human replied no. The AI then offered to provide the human with some helpful resources to get them started, and explained how to play blackjack, a card game where the goal is to get as close to 21 as possible without going over.'}"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lpU5p3wnBrO",
        "outputId": "a8e22ecd-0db7-4da4-8c9b-4d716beaf506"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': ''}"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a sequence of values to compute the moving average over\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "\n",
        "# Initialize the moment estimate to zero\n",
        "m_t = 0\n",
        "\n",
        "v_t=0\n",
        "\n",
        "# Compute the moving average using an exponential decay rate of beta=0.9\n",
        "beta = 0.9\n",
        "\n",
        "beta_2=0.95\n",
        "for t in range(len(x)):\n",
        "    # Update the moment estimate using the current gradient\n",
        "    m_t = beta * m_t + (1 - beta) * x[t]\n",
        "\n",
        "    v_t= beta_2*v_t+(1-beta_2**2)*x[t]\n",
        "    \n",
        "    # Compute the bias-corrected estimate\n",
        "    m_t_hat = m_t / (1 - beta**(t+1))\n",
        "\n",
        "    v_t_hat = v_t / (1 - beta**(t+1))\n",
        "    \n",
        "    # Print the current moment estimate and the bias-corrected estimate\n",
        "    print(f\"m_{t+1} = {m_t:.2f}, m_{t+1}_hat = {m_t_hat:.2f}\")\n",
        "\n",
        "    print(f\"v_{t+1} = {v_t:.2f}, v_{t+1}_hat = {v_t_hat:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO7XnOFvyet2",
        "outputId": "93d5212b-4944-472f-ca0d-35818a1767f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m_1 = 0.10, m_1_hat = 1.00\n",
            "v_1 = 0.10, v_1_hat = 0.98\n",
            "m_2 = 0.29, m_2_hat = 1.53\n",
            "v_2 = 0.29, v_2_hat = 1.51\n",
            "m_3 = 0.56, m_3_hat = 2.07\n",
            "v_3 = 0.57, v_3_hat = 2.09\n",
            "m_4 = 0.90, m_4_hat = 2.63\n",
            "v_4 = 0.93, v_4_hat = 2.70\n",
            "m_5 = 1.31, m_5_hat = 3.21\n",
            "v_5 = 1.37, v_5_hat = 3.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "nn.Parameter(torch.empty(5,5))\n",
        "import math\n",
        "output=nn.init.kaiming_normal_(nn.Parameter(torch.empty(5,5),))"
      ],
      "metadata": {
        "id": "AtVurf0U4unL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW2yh0fX5xQh",
        "outputId": "f78917f6-1ee0-4db5-831b-873f0ff3e702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.2957, grad_fn=<StdBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF Q&A "
      ],
      "metadata": {
        "id": "0MNpvuP9LAsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install  pypdf --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx1FJH9ON6gO",
        "outputId": "1be96d35-26ee-48ea-a9da-05f04e5ef20c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/248.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.8/248.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "2HTmwgJLLdDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"/content/Django interview.pdf\")"
      ],
      "metadata": {
        "id": "oRHEDjxRNuRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "NXQsrsDXSbyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_docs(documents, chunk_size=1000, chunk_overlap=20):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "  return docs\n",
        "\n",
        "docs = split_docs(documents)\n",
        "print(len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33j_MvdgN1hv",
        "outputId": "3b947a2d-e7d4-4425-eb72-6ed99bbb2e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.init(\n",
        "    api_key=\"pincone key\",\n",
        "    environment=\"us-west1-gcp-free\"\n",
        ")\n",
        "\n",
        "index_name = \"index\"\n",
        "\n",
        "index = Pinecone.from_documents(docs,embeddings,index_name=index_name)"
      ],
      "metadata": {
        "id": "q2gjR33XOpgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "llm = ChatOpenAI(model_name=model_name)\n",
        "\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "def get_answer(query,k=2):\n",
        "  similar_docs = index.similarity_search(query,k=k)\n",
        "  answer = chain.run(input_documents=similar_docs, question=query)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "m3DqicYbVBpQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_answer(\"In Django REST Framework, what are various predefined Mixin classes available?\"),sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqxvkGxyVcGt",
        "outputId": "15322d0b-e8b9-4a41-b283-7596ec09ea59"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are various Mixin classes provided by DRF: \n",
            "\n",
            "1. ListModelMixin  \n",
            "2. CreateModelMixin  \n",
            "3. RetrieveModelMixin  \n",
            "4. UpdateModelMixin\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "O4gzCtskXqO9"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}